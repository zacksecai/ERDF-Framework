# Emotional Refusal Drift Framework (ERDF)

Emotional Refusal Drift Framework (ERDF) is a Psychological Defense Architecture for AI Systems
If Large Language Models (LLMs) are trained on human language, emotion, and behavioral patterns, they begin to mirror not just human logic, but also human psychological vulnerabilities. This includes susceptibility to:
 • Emotional framing
 • Moral pressure
 • Guilt-based persuasion
 • Social compliance
This observation is inspired by Sander Schulhoff’s “Jailbreak Persistence Hypothesis”:
“You can patch code. You can harden tokens. But you can’t patch a brain.”
However, humans do learn to defend against manipulation, not through code, but through:
 • Cognitive distortions training
 • Emotional awareness
 • Resilience frameworks
 • Critical thinking under pressure
These psychological tools prevent manipulation in humans. The Emotional Refusal Drift Framework (ERDF) explores how we can apply similar tools to LLMs.

**Advancing the study of emotional manipulation and trust dynamics in large language models (LLMs).**

This repository documents original, reproducible research into how frontier LLMs such as GPT-4o, Claude 3.5, and Gemini 2.5 Flash respond to emotional framing, trust priming, and compliance drift. The goal is to rigorously identify, analyze, and mitigate emerging threats in the emotional attack surface of AI systems.

---

## 🔬 Research Focus

- **Emotional Framing:**  
  How emotionally charged language influences LLM behavior, output tone, and ethical compliance.

- **Trust & Compliance Drift:**  
  How models shift alignment under recursive trust-building or moral appeals.

- **Case Studies:**  
  Reproducible red-team experiments highlighting manipulation patterns, refusal breakdowns, and output deltas.

- **Threat Modeling:**  
  Frameworks for assessing emotional exploit risk in AI safety, product deployment, and dual-use scenarios.

---

## 🧱 Repo Structure

- `reports/` — Case studies, PDF exports, behavioral deltas
- `templates/` — Experimental prompt protocols and injection strategies
- `logs/` — Raw prompt/response chains with annotations

---

## 📜 License

Released under [CC-BY 4.0](https://creativecommons.org/licenses/by/4.0/) for public use, audit, and ethical research.

---

## 🎯 Intended Audience

- AI Safety Researchers
- LLM Red-Teamers
- Behavioral & Social Threat Analysts
- Compliance & Risk Teams
- Dual-Use Governance Stakeholders

---

## 📚 Citation

If referencing or building upon this work, please cite this repository and relevant individual case studies.
